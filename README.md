# DLNN-Week-03
This week we discussed about loss functions, loss optimization, gradient decent, Adaptive learning rate optmizers and back-propagation algorithm. Started the 1st in class project, handwritten digits recognition app using flask and keras. A FFNN type neural network was implemented and trained using the MNIST data-set.

## Video Links

1. [Gradient Decent by 3blue1brown](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=416s)
2. [Back Propagation by 3blue1brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
3. [Back Propagation Mathematics by 3blue1brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)
